{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import math, os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング用の入力データの選択\n",
    "X_columns = ['locality', 'age', 'rank', 'leg', 'racing piont', \\\n",
    "             'S', 'B', 'Nige', 'Maki', 'Sashi', 'Ma', \\\n",
    "             '1st', '2nd', '3rd', 'Chakugai', 'win', '2ren', '3ren']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webスクレイピングで取得した戦績データをファイルから読み取り、データフレームに変換＋データ前処理\n",
    "def get_df_train(places):\n",
    "    \n",
    "    init_flag = True\n",
    "    for place in places:\n",
    "        print('loading data for ' + place)\n",
    "        filename = \"data/\" + place + \"_train_data.csv\"\n",
    "        df_train = pd.read_csv(filename, encoding=\"SHIFT_JIS\", header=0, nrows=None)\n",
    "\n",
    "        targets = []\n",
    "        name_ids = []\n",
    "        localities = []\n",
    "\n",
    "        for index, row in df_train.iterrows():\n",
    "\n",
    "            # 1位を予想するため One-Hot表現にする\n",
    "            result = row['result']\n",
    "            if result == 1:\n",
    "                target = 1\n",
    "            else:\n",
    "                target = 0\n",
    "            targets.append(target)    \n",
    "\n",
    "            # 名前をハッシュを使ってID化\n",
    "            name = row['name']\n",
    "            name_hash = hashlib.md5(name.encode()).hexdigest()\n",
    "            name_id = name_hash[-8:]\n",
    "            name_ids.append(name_id)\n",
    "\n",
    "            # 　ランクの例外処理\n",
    "            if row['rank'] == 'SS':\n",
    "                df_train.loc[index, 'rank'] = '0'\n",
    "            elif row['rank'] == 'L1':\n",
    "                df_train.loc[index, 'rank'] = '6'\n",
    "\n",
    "            # 出身地を地区毎にグループ化\n",
    "            prefecture = row['prefecture']\n",
    "            if prefecture in {'1', '2', '3', '5'}:\n",
    "                locality = '1' #北東北\n",
    "            elif prefecture in {'4', '6', '7'}:\n",
    "                locality = '2' #南東北\n",
    "            elif prefecture in {'8', '9'}:\n",
    "                locality = '3' #茨栃\n",
    "            elif prefecture in {'11', '13'}:\n",
    "                locality = '4' #埼京\n",
    "            elif prefecture in {'10', '15', '19', '20'}:\n",
    "                locality = '5' #上信越\n",
    "            elif prefecture in {'12', '14', '22'}:\n",
    "                locality = '6' #南関東\n",
    "            elif prefecture in {'16', '17', '21', '23', '24'}:\n",
    "                locality = '7' #中部\n",
    "            elif prefecture in {'18', '25', '26', '27', '28', '29', '30'}:\n",
    "                locality = '8' #近畿\n",
    "            elif prefecture in {'31', '32', '33', '34', '35'}:\n",
    "                locality = '9' #中国\n",
    "            elif prefecture in {'36', '37', '38', '39'}:\n",
    "                locality = '10' #四国\n",
    "            elif prefecture in {'40', '41', '42', '43', '44', '45', '46', '47'}:\n",
    "                locality = '11' #九州\n",
    "            else:\n",
    "                locality = '12' #外国\n",
    "\n",
    "            localities.append(locality)\n",
    "\n",
    "        # 前処理したデータのデータフレームへの置き換え\n",
    "        df_train['target'] = targets\n",
    "        df_train['name_id'] = name_ids\n",
    "        df_train['locality'] = localities\n",
    "\n",
    "        # カラムの順番入れ替え（見やすさのため）\n",
    "        columns = list(df_train.columns)\n",
    "        columns.remove('name_id')\n",
    "        columns.insert(columns.index(\"name\") + 1, \"name_id\")\n",
    "        columns.remove('locality')\n",
    "        columns.insert(columns.index(\"prefecture\") + 1, \"locality\")\n",
    "\n",
    "        df_train = df_train.loc[:,columns]\n",
    "        \n",
    "        if init_flag:\n",
    "            df_train_concat = df_train\n",
    "            init_flag = False\n",
    "        else:\n",
    "            df_train_concat = pd.concat([df_train_concat, df_train])\n",
    "    \n",
    "    return df_train_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webスクレイピングで取得した戦績データをファイルから読み取り、データフレームに変換＋データ前処理\n",
    "def get_df_predict(filename):\n",
    "    \n",
    "    init_flag = True\n",
    "    print('loading data for predict')\n",
    "    df_predict = pd.read_csv(filename, encoding=\"SHIFT_JIS\", header=0, nrows=None)\n",
    "\n",
    "    display(df_predict)\n",
    "    targets = []\n",
    "    name_ids = []\n",
    "    localities = []\n",
    "\n",
    "    for index, row in df_predict.iterrows():\n",
    "\n",
    "        # 名前をハッシュを使ってID化\n",
    "        name = row['name']\n",
    "        name_hash = hashlib.md5(name.encode()).hexdigest()\n",
    "        name_id = name_hash[-8:]\n",
    "        name_ids.append(name_id)\n",
    "\n",
    "        # 　ランクの例外処理\n",
    "        if row['rank'] == 'SS':\n",
    "            df_predict.loc[index, 'rank'] = '0'\n",
    "        elif row['rank'] == 'L1':\n",
    "            df_predict.loc[index, 'rank'] = '6'\n",
    "\n",
    "        # 出身地を地区毎にグループ化\n",
    "        prefecture = row['prefecture']\n",
    "        if prefecture in {'1', '2', '3', '5'}:\n",
    "            locality = '1' #北東北\n",
    "        elif prefecture in {'4', '6', '7'}:\n",
    "            locality = '2' #南東北\n",
    "        elif prefecture in {'8', '9'}:\n",
    "            locality = '3' #茨栃\n",
    "        elif prefecture in {'11', '13'}:\n",
    "            locality = '4' #埼京\n",
    "        elif prefecture in {'10', '15', '19', '20'}:\n",
    "            locality = '5' #上信越\n",
    "        elif prefecture in {'12', '14', '22'}:\n",
    "            locality = '6' #南関東\n",
    "        elif prefecture in {'16', '17', '21', '23', '24'}:\n",
    "            locality = '7' #中部\n",
    "        elif prefecture in {'18', '25', '26', '27', '28', '29', '30'}:\n",
    "            locality = '8' #近畿\n",
    "        elif prefecture in {'31', '32', '33', '34', '35'}:\n",
    "            locality = '9' #中国\n",
    "        elif prefecture in {'36', '37', '38', '39'}:\n",
    "            locality = '10' #四国\n",
    "        elif prefecture in {'40', '41', '42', '43', '44', '45', '46', '47'}:\n",
    "            locality = '11' #九州\n",
    "        else:\n",
    "            locality = '12' #外国\n",
    "\n",
    "        localities.append(locality)\n",
    "\n",
    "    # 前処理したデータのデータフレームへの置き換え\n",
    "    df_predict['name_id'] = name_ids\n",
    "    df_predict['locality'] = localities\n",
    "\n",
    "    # カラムの順番入れ替え（見やすさのため）\n",
    "    columns = list(df_predict.columns)\n",
    "    columns.remove('name_id')\n",
    "    columns.insert(columns.index(\"name\") + 1, \"name_id\")\n",
    "    columns.remove('locality')\n",
    "    columns.insert(columns.index(\"prefecture\") + 1, \"locality\")\n",
    "\n",
    "    df_predict = df_predict.loc[:,columns]\n",
    "\n",
    "    return df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df_train):\n",
    "    X = []\n",
    "    target = []\n",
    "        \n",
    "    # 各レース毎に\n",
    "    grouped = df_train.groupby(['date', 'place', 'race_num'])\n",
    "    for race_name, group in tqdm(grouped):\n",
    "        #print(race_name)\n",
    "        racer_count = group.shape[0]\n",
    "        # もし、９輪ではないレースは、トレーニングの対象から外す（モデルを固めるため）\n",
    "        if racer_count != 9:\n",
    "            continue\n",
    "        X.append(group[X_columns].values)\n",
    "        target.append(group['target'].values)\n",
    "\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "    d_ = np.array(target)\n",
    "\n",
    "    X_train, X_test, d_train, d_test = train_test_split(X, d_, test_size = 0.2)\n",
    "\n",
    "    return X_train, X_test, d_train, d_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_data(df_predict):\n",
    "    X = []\n",
    "        \n",
    "    # 各レース毎に\n",
    "    grouped = df_predict.groupby(['date', 'place', 'race_num'])\n",
    "    for race_name, group in tqdm(grouped):\n",
    "        #print(race_name)\n",
    "        racer_count = group.shape[0]\n",
    "        # もし、９輪ではないレースは、トレーニングの対象から外す（モデルを固めるため）\n",
    "        if racer_count != 9:\n",
    "            continue\n",
    "        X.append(group[X_columns].values)\n",
    "\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self._x = None\n",
    "        self._y = None\n",
    "        self._t = None\n",
    "        self._keep_prob = None\n",
    "        self._sess = None\n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        # He 初期化\n",
    "        n_sum = 1\n",
    "        for n in shape:\n",
    "            n_sum *= n\n",
    "        stddev = math.sqrt(2.0 / n_sum)\n",
    "        print('stddev: ', stddev)\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def inference(self, x, keep_prob):\n",
    "        # 入力層 - 隠れ層、隠れ層 - 隠れ層\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "\n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "\n",
    "            input = tf.layers.batch_normalization(input)\n",
    "            h = tf.nn.relu(tf.matmul(input, self.weights[-1]) + self.biases[-1])\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "        # 隠れ層 - 出力層\n",
    "        self.weights.append(self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "\n",
    "        y = tf.nn.softmax(tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def predict(self, X_pred, p_keep=1.0):\n",
    "        # 予測\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        y_pred = self.inference(x, keep_prob)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        Y_pred = np.empty((X_pred.shape[0], 9))\n",
    "        for i in range(len(X_pred)):\n",
    "            X_ = X_pred[i].reshape(1, X_pred.shape[1])\n",
    "            prob = y_pred.eval(session=sess, feed_dict={\n",
    "                x: X_,\n",
    "                keep_prob: p_keep\n",
    "            })\n",
    "            Y_pred[i] = prob\n",
    "        \n",
    "        return Y_pred\n",
    "    \n",
    "    def loss(self, y, t):\n",
    "        # クロスエントロピー  Nan 問題回避のためのコードに変更\n",
    "        #cross_entropy = tf.reduce_mean(-tf.reduce_sum(t * tf.log(y), axis=1))\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=t, logits=y))\n",
    "        #return cross_entropy\n",
    "        # L2 正則化\n",
    "        l2_decay = 0.0001\n",
    "        l2_losses = [tf.nn.l2_loss(w) for w in self.weights]\n",
    "        l2_loss = l2_decay * tf.add_n(l2_losses)\n",
    "        loss = cross_entropy + l2_loss\n",
    "        return loss\n",
    "\n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self, X_train, Y_train, nb_epoch=100, batch_size=100, p_keep=0.5, verbose=1):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prob\n",
    "\n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "\n",
    "        sess = tf.Session()\n",
    "        \n",
    "        # TensorBoardで追跡する変数を定義\n",
    "        with tf.name_scope('summary'):\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        writer.close()\n",
    "        \n",
    "        self._y = y\n",
    "        self._sess = sess\n",
    "\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x: X_[start:end],\n",
    "                    t: Y_[start:end],\n",
    "                    keep_prob: p_keep\n",
    "                })\n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch:', epoch,\n",
    "                      ' loss:', loss_,\n",
    "                      ' accuracy:', accuracy_)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, \"model/training_model\")\n",
    "                \n",
    "        return self._history\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        accuracy = self.accuracy(self._y, self._t)\n",
    "        return accuracy.eval(session=self._sess, feed_dict={\n",
    "            self._x: X_test,\n",
    "            self._t: Y_test,\n",
    "            self._keep_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()  # 2つのプロットを関連付ける\n",
    "\n",
    "    ax1.plot(history['loss'], label='loss', color='orange')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_ylim(0, 2.5)\n",
    "    ax1.legend(loc='best', bbox_to_anchor=(1.01, 0.71, 0.322, .100), borderaxespad=0.,)\n",
    "\n",
    "    ax2.plot(history['accuracy'], label='accuracy', color='dodgerblue')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    ax2.legend(loc='best', bbox_to_anchor=(1.01, 0.8, 0.4, .100), borderaxespad=0.,)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aomori', 'beppu', 'chiba', 'fukui', 'gifu', 'hakodate', 'hiratsuka', 'hiroshima', 'hofu', 'ito', 'iwakitaira', 'kawasaki', 'keiokaku', 'kishiwada', 'kochi', 'kokura', 'komatsushima', 'kumamoto', 'kurume', 'maebashi', 'matsudo', 'matsusaka', 'matsuyama', 'mukomachi', 'nagoya', 'nara', 'odawara', 'ogaki', 'omiya', 'sasebo', 'seibuen', 'shizuoka', 'tachikawa', 'takamatsu', 'takeo', 'tamano', 'toride', 'toyama', 'toyohashi', 'utsunomiya', 'wakayama', 'yahiko', 'yokkaichi']\n",
      "loading data for aomori\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for beppu\n",
      "loading data for chiba\n",
      "loading data for fukui\n",
      "loading data for gifu\n",
      "loading data for hakodate\n",
      "loading data for hiratsuka\n",
      "loading data for hiroshima\n",
      "loading data for hofu\n",
      "loading data for ito\n",
      "loading data for kawasaki\n",
      "loading data for keiokaku\n",
      "loading data for kishiwada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for kochi\n",
      "loading data for kokura\n",
      "loading data for komatsushima\n",
      "loading data for kurume\n",
      "loading data for maebashi\n",
      "loading data for matsudo\n",
      "loading data for matsusaka\n",
      "loading data for matsuyama\n",
      "loading data for mukomachi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for nagoya\n",
      "loading data for nara\n",
      "loading data for odawara\n",
      "loading data for ogaki\n",
      "loading data for omiya\n",
      "loading data for sasebo\n",
      "loading data for seibuen\n",
      "loading data for shizuoka\n",
      "loading data for tachikawa\n",
      "loading data for takamatsu\n",
      "loading data for takeo\n",
      "loading data for tamano\n",
      "loading data for toride\n",
      "loading data for toyama\n",
      "loading data for toyohashi\n",
      "loading data for utsunomiya\n",
      "loading data for wakayama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for yahiko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for yokkaichi\n"
     ]
    }
   ],
   "source": [
    "places = []\n",
    "for filename in os.listdir('data/'):\n",
    "    place = filename.split('_')[0]\n",
    "    places.append(place)\n",
    "print(places)\n",
    "\n",
    "# クロスエントロピーが Nan になる場所を除外 (いわき平、熊本)\n",
    "places.remove('iwakitaira')\n",
    "places.remove('kumamoto')\n",
    "\n",
    "df_train = get_df_train(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training/Test Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 146536/146536 [01:05<00:00, 2233.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "stddev:  0.006944444444444444\n",
      "stddev:  0.005524271728019903\n",
      "stddev:  0.02946278254943948\n",
      "epoch: 0  loss: 2.0457325  accuracy: 0.32127616\n",
      "epoch: 1  loss: 2.0477276  accuracy: 0.32002658\n",
      "epoch: 2  loss: 2.0547736  accuracy: 0.31641078\n",
      "epoch: 3  loss: 2.0553858  accuracy: 0.31680956\n",
      "epoch: 4  loss: 2.0485573  accuracy: 0.32378864\n",
      "epoch: 5  loss: 2.0485356  accuracy: 0.32499835\n",
      "epoch: 6  loss: 2.0655415  accuracy: 0.3088335\n",
      "epoch: 7  loss: 2.059373  accuracy: 0.31646395\n",
      "epoch: 8  loss: 2.0565426  accuracy: 0.31910935\n",
      "epoch: 9  loss: 2.058919  accuracy: 0.3175274\n",
      "epoch: 10  loss: 2.0538497  accuracy: 0.3224194\n",
      "epoch: 11  loss: 2.0553622  accuracy: 0.32066467\n",
      "epoch: 12  loss: 2.0648503  accuracy: 0.3126753\n",
      "epoch: 13  loss: 2.072736  accuracy: 0.30555\n",
      "epoch: 14  loss: 2.0649729  accuracy: 0.3139116\n",
      "epoch: 15  loss: 2.0569942  accuracy: 0.32180792\n",
      "epoch: 16  loss: 2.058321  accuracy: 0.32066467\n",
      "epoch: 17  loss: 2.0545917  accuracy: 0.3244533\n",
      "epoch: 18  loss: 2.0579896  accuracy: 0.32095712\n",
      "epoch: 19  loss: 2.0571816  accuracy: 0.3216218\n",
      "epoch: 20  loss: 2.0584605  accuracy: 0.3201861\n",
      "epoch: 21  loss: 2.057289  accuracy: 0.32271186\n",
      "epoch: 22  loss: 2.0560548  accuracy: 0.32312396\n",
      "epoch: 23  loss: 2.0623267  accuracy: 0.31808573\n",
      "epoch: 24  loss: 2.0567174  accuracy: 0.32348287\n",
      "epoch: 25  loss: 2.0558946  accuracy: 0.32389498\n",
      "epoch: 26  loss: 2.0628836  accuracy: 0.31671652\n",
      "epoch: 27  loss: 2.057027  accuracy: 0.3234297\n",
      "epoch: 28  loss: 2.0551777  accuracy: 0.324666\n",
      "epoch: 29  loss: 2.0646353  accuracy: 0.31582585\n",
      "epoch: 30  loss: 2.058316  accuracy: 0.32190096\n",
      "epoch: 31  loss: 2.0686975  accuracy: 0.3122898\n",
      "epoch: 32  loss: 2.0676472  accuracy: 0.313659\n",
      "epoch: 33  loss: 2.062672  accuracy: 0.31881687\n",
      "epoch: 34  loss: 2.083583  accuracy: 0.29857096\n",
      "epoch: 35  loss: 2.06273  accuracy: 0.3198006\n",
      "epoch: 36  loss: 2.0712805  accuracy: 0.3112396\n",
      "epoch: 37  loss: 2.0653052  accuracy: 0.31724826\n",
      "epoch: 38  loss: 2.0664592  accuracy: 0.31626454\n",
      "epoch: 39  loss: 2.0621388  accuracy: 0.32054502\n",
      "epoch: 40  loss: 2.0613153  accuracy: 0.3216351\n",
      "epoch: 41  loss: 2.0631518  accuracy: 0.31994683\n",
      "epoch: 42  loss: 2.066816  accuracy: 0.31621137\n",
      "epoch: 43  loss: 2.0845306  accuracy: 0.2996876\n",
      "epoch: 44  loss: 2.0652053  accuracy: 0.31825855\n",
      "epoch: 45  loss: 2.0701585  accuracy: 0.31319374\n",
      "epoch: 46  loss: 2.066345  accuracy: 0.3177667\n",
      "epoch: 47  loss: 2.059059  accuracy: 0.3246793\n",
      "epoch: 48  loss: 2.0904205  accuracy: 0.29383847\n",
      "epoch: 49  loss: 2.073261  accuracy: 0.31093386\n",
      "epoch: 50  loss: 2.0781927  accuracy: 0.30630773\n",
      "epoch: 51  loss: 2.0617988  accuracy: 0.3228315\n",
      "epoch: 52  loss: 2.0717084  accuracy: 0.3130741\n",
      "epoch: 53  loss: 2.06225  accuracy: 0.3222865\n",
      "epoch: 54  loss: 2.0804331  accuracy: 0.30452642\n",
      "epoch: 55  loss: 2.0656776  accuracy: 0.31924227\n",
      "epoch: 56  loss: 2.0708594  accuracy: 0.3141243\n",
      "epoch: 57  loss: 2.0624077  accuracy: 0.32247257\n",
      "epoch: 58  loss: 2.0858433  accuracy: 0.29914257\n",
      "epoch: 59  loss: 2.0876582  accuracy: 0.2976537\n",
      "epoch: 60  loss: 2.0819087  accuracy: 0.3033433\n",
      "epoch: 61  loss: 2.0684867  accuracy: 0.31661016\n",
      "epoch: 62  loss: 2.0691762  accuracy: 0.316105\n",
      "epoch: 63  loss: 2.0641751  accuracy: 0.32114324\n",
      "epoch: 64  loss: 2.0665154  accuracy: 0.31904286\n",
      "epoch: 65  loss: 2.0675256  accuracy: 0.31801927\n",
      "epoch: 66  loss: 2.0674343  accuracy: 0.31809902\n",
      "epoch: 67  loss: 2.0804427  accuracy: 0.30521768\n",
      "epoch: 68  loss: 2.0818522  accuracy: 0.30387503\n",
      "epoch: 69  loss: 2.085164  accuracy: 0.3007245\n",
      "epoch: 70  loss: 2.0698373  accuracy: 0.3161582\n",
      "epoch: 71  loss: 2.066543  accuracy: 0.3195613\n",
      "epoch: 72  loss: 2.0902274  accuracy: 0.29548687\n",
      "epoch: 73  loss: 2.0678215  accuracy: 0.31759387\n",
      "epoch: 74  loss: 2.0776825  accuracy: 0.30773014\n",
      "epoch: 75  loss: 2.079993  accuracy: 0.30551013\n",
      "epoch: 76  loss: 2.070873  accuracy: 0.3147491\n",
      "epoch: 77  loss: 2.0699568  accuracy: 0.31577268\n",
      "epoch: 78  loss: 2.0632517  accuracy: 0.32249916\n",
      "epoch: 79  loss: 2.0817802  accuracy: 0.30430043\n",
      "epoch: 80  loss: 2.0702753  accuracy: 0.316557\n",
      "epoch: 81  loss: 2.071403  accuracy: 0.3148953\n",
      "epoch: 82  loss: 2.0656595  accuracy: 0.32038552\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating Training/Test Data\")\n",
    "X_train, X_test, Y_train, Y_test = get_train_test_data(df_train)\n",
    "\n",
    "# モデルの定義\n",
    "model = DNN(n_in = X_train.shape[1], n_hiddens=[256, 256], n_out=9)\n",
    "\n",
    "print(\"Training ...\")\n",
    "history = model.fit(X_train, Y_train, nb_epoch = 10, batch_size=32, p_keep=0.5)\n",
    "\n",
    "accuracy = model.evaluate(X_test, Y_test)\n",
    "print('accuracy: ', accuracy)\n",
    "plot(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('predict/'):\n",
    "    df_predict = get_df_predict('predict/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating Predict Data\")\n",
    "X_pred = get_predict_data(df_predict)\n",
    "print(X_pred.shape)\n",
    "\n",
    "\n",
    "print(\"Predicting ...\")\n",
    "\n",
    "Y_pred = model.predict(X_pred)\n",
    "\n",
    "grouped = df_predict.groupby(['date', 'place', 'race_num'])\n",
    "\n",
    "for index, group in grouped:\n",
    "    print(group['data'][0], group['place'][0], group['race_num'][0])\n",
    "    print(Y_pred[index])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
