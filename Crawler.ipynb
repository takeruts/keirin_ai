{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_url(target_month, place):\n",
    "    year = target_month.strftime(\"%Y\")\n",
    "    month = target_month.strftime(\"%m\")\n",
    "    base = 'https://keirin.kdreams.jp'\n",
    "    baseAdd1 = \"/\" + place + \"/schedule/\" + year + \"/\" + month\n",
    "    url = base + baseAdd1\n",
    "    return urllib.parse.quote_plus(url, \"/:?=&\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_span(start_month, end_month):\n",
    "    \"\"\"start_date、end_dateの期間に含まれる日毎のdatetimeオブジェクトを返すジェネレータ\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while 1 :\n",
    "        target_month = start_month + relativedelta(months=i)\n",
    "        if target_month > end_month: break\n",
    "        yield target_month\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_racedetail_to_csv(racedetail_links, kaisaidate, place, filename):\n",
    "\n",
    "    for i, racedetail_link in enumerate(racedetail_links):\n",
    "\n",
    "        race_num = str(i + 1)\n",
    "        \n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        time.sleep(sleep_time)\n",
    "        try:\n",
    "            response = requests.get(racedetail_link, headers=headers)\n",
    "        except:\n",
    "            print(\"Response error:\", racedetail_link)\n",
    "            continue\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        print(\"RACE RESULT\", kaisaidate, place, race_num)\n",
    "\n",
    "        result_bs = soup.find(\"table\", class_=\"result_table\")\n",
    "        trs = result_bs.find_all(\"tr\")\n",
    "        trs.pop(0)\n",
    "        race_result = []\n",
    "        for tr in trs:\n",
    "            result_rows = []\n",
    "            tds = tr.find_all(\"td\")\n",
    "            for td in tds:\n",
    "                result_rows.append(td.text)\n",
    "            #print(result_rows)\n",
    "            # make list to store result by order\n",
    "            race_result.append(result_rows[2])\n",
    "\n",
    "        print(race_result)\n",
    "\n",
    "        print(\"RACE RACORD\")\n",
    "        racecard_bs = soup.find(\"table\", class_=\"racecard_table\")\n",
    "        #print(racecard)\n",
    "        riders_bs = racecard_bs.find_all(\"tr\", class_=re.compile(\"^n\"))\n",
    "        for rider_bs in riders_bs:\n",
    "            rows = []\n",
    "            for td in rider_bs.find_all(\"td\"):\n",
    "                row = td.text\n",
    "                row = ''.join(row.split())\n",
    "                rows.append(row)\n",
    "            if len(rows) < 23:\n",
    "                rows.insert(3, bracket)\n",
    "\n",
    "            names = rider_bs.find(\"td\", class_=\"rider bdr_r\").get_text().strip().splitlines()\n",
    "            name = names[0]\n",
    "            homes = ''.join(names[1].split()).split(\"/\")\n",
    "            prefecture = homes[0]\n",
    "            age = homes[1]\n",
    "            period = homes[2]\n",
    "\n",
    "            rows.pop(5)\n",
    "            rows.insert(5, name)\n",
    "            rows.insert(6, prefecture)\n",
    "            rows.insert(7, age)\n",
    "            rows.insert(8, period)\n",
    "\n",
    "            #print(rows)\n",
    "\n",
    "            rows[0] = rows[0].replace('◎', '1').replace('○', '2').replace('△', '3').replace('▲', '4').replace('×', '5').replace('注', '6')\n",
    "            bracket = rows[3]\n",
    "            rows[6] = rows[6].replace('北海道', '1').replace('青森', '2').replace('岩手', '3').replace('宮城', '4').replace('秋田', '5').replace('山形', '6').replace('福島', '7').replace('茨城', '8').replace('栃木', '9').replace('群馬', '10').replace('埼玉', '11').replace('千葉', '12').replace('東京', '13').replace('神奈川', '14').replace('新潟', '15').replace('富山', '16').replace('石川', '17').replace('福井', '18').replace('山梨', '19').replace('長野', '20').replace('岐阜', '21').replace('静岡', '22').replace('愛知', '23').replace('三重', '24').replace('滋賀', '25').replace('京都', '26').replace('大阪', '27').replace('兵庫', '28').replace('奈良', '29').replace('和歌山', '30').replace('鳥取', '31').replace('島根', '32').replace('岡山', '33').replace('広島', '34').replace('山口', '35').replace('徳島', '36').replace('香川', '37').replace('愛媛', '38').replace('高知', '39').replace('福岡', '40').replace('佐賀', '41').replace('長崎', '42').replace('熊本', '43').replace('大分', '44').replace('宮崎', '45').replace('鹿児島', '46').replace('沖縄', '47')\n",
    "            rows[9] = rows[9].replace('S1', '1').replace('S2', '2').replace('A1', '3').replace('A2', '4').replace('A3', '5')\n",
    "            rows[10] = rows[10].replace('両', '1').replace('逃', '2').replace('追', '3')\n",
    "\n",
    "            car_num = rows[4]\n",
    "            result = race_result.index(car_num) + 1\n",
    "            rows.append(result)\n",
    "\n",
    "            rows.insert(0, kaisaidate)\n",
    "            rows.insert(1, place)\n",
    "            rows.insert(2, race_num)\n",
    "\n",
    "            print(rows)\n",
    "\n",
    "            with open(filename, \"a\") as f:\n",
    "                    writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "                    writer.writerow(rows)     # list（1次元配列）の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"test.csv\"\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "    \n",
    "csv_header = [''] * 30\n",
    "csv_header[0] = 'date'\n",
    "csv_header[1] = 'place'\n",
    "csv_header[2] = 'race_num'\n",
    "csv_header[3] = 'predict'\n",
    "csv_header[4] = 'koukiai'\n",
    "csv_header[5] = 'evaluation'\n",
    "csv_header[6] = 'bracket'\n",
    "csv_header[7] = 'car_num'\n",
    "csv_header[8] = 'name'\n",
    "csv_header[9] = 'prefecture'\n",
    "csv_header[10] = 'age'\n",
    "csv_header[11] = 'period'\n",
    "csv_header[12] = 'rank'\n",
    "csv_header[13] = 'leg'\n",
    "csv_header[14] = 'gear'\n",
    "csv_header[15] = 'racing piont'\n",
    "csv_header[16] = 'S'\n",
    "csv_header[17] = 'B'\n",
    "csv_header[18] = 'Nige'\n",
    "csv_header[19] = 'Maki'\n",
    "csv_header[20] = 'Sashi'\n",
    "csv_header[21] = 'Ma'\n",
    "csv_header[22] = '1st'\n",
    "csv_header[23] = '2nd'\n",
    "csv_header[24] = '3rd'\n",
    "csv_header[25] = 'Chakugai'\n",
    "csv_header[26] = 'win'\n",
    "csv_header[27] = '2ren'\n",
    "csv_header[28] = '3ren'\n",
    "csv_header[29] = 'result'\n",
    "\n",
    "with open(filename, \"a\") as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n') # 改行コード（\\n）を指定しておく\n",
    "        writer.writerow(csv_header)   # list（1次元配列）の場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 5 seconds before access url\n",
    "sleep_time = 5\n",
    "\n",
    "start_month = datetime.strptime('201501', '%Y%m')\n",
    "end_month = datetime.strptime('201808', '%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = ['hiratsuka']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_month in month_span(start_month, end_month):\n",
    "    for place in places:\n",
    "        target_url = get_data_url(target_month, place)\n",
    "        print(target_url)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        time.sleep(sleep_time)\n",
    "        try:\n",
    "            response = requests.get(target_url, headers=headers)# <Response [200]>\n",
    "        except:\n",
    "            print(\"Response error:\", target_url)\n",
    "            continue\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        a_results = soup.find_all(\"a\", string=\"結果\")\n",
    "\n",
    "        for a_result in a_results:\n",
    "            raceresult_link = a_result.get(\"href\")\n",
    "            print(raceresult_link)\n",
    "            time.sleep(sleep_time)\n",
    "            try:\n",
    "                response = requests.get(raceresult_link, headers=headers)\n",
    "            except:\n",
    "                print(\"Response error:\", raceresult_link)\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            lis = soup.find(\"ul\", id=\"JS_UL_KAISAI_DATE_TAB\").find_all(\"li\")\n",
    "            kaisaidates = []\n",
    "            for li in lis:\n",
    "                kaisaidates.append(li.get(\"kaisaidate\"))\n",
    "\n",
    "            for kaisaidate in kaisaidates:\n",
    "                # get race detail base URL\n",
    "                racedetail_id = \"JS_DL_KAISAI_DETAIL_INFO_NAV_\" + kaisaidate\n",
    "                racedetail_link_base = soup.find(\"dl\", id=racedetail_id).find(\"a\", string=\"レース詳細\").get(\"href\")\n",
    "                print(racedetail_link_base)\n",
    "\n",
    "                # get racedetail URLs of all races\n",
    "                time.sleep(sleep_time)\n",
    "                try:\n",
    "                    response = requests.get(racedetail_link_base, headers=headers)\n",
    "                except:\n",
    "                    print(\"Response error:\", racedetail_link_base)\n",
    "                    continue\n",
    "                    \n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                dl_id = \"JS_DL_KAISAI_DETAIL_RACE_NAV_\" + kaisaidate\n",
    "                a_races = soup.find(\"dl\", id=dl_id).find_all(\"a\")\n",
    "\n",
    "                racedetail_links = []\n",
    "                for a_race in a_races:\n",
    "                    href = a_race.get(\"href\")\n",
    "                    racedetail_links.append(href)\n",
    "                racedetail_links[0] = racedetail_link_base\n",
    "                \n",
    "                # call function - parse racedetail URL and save racer data and result to csv file\n",
    "                parse_racedetail_to_csv(racedetail_links, kaisaidate, place, filename)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"test.csv\", encoding=\"SHIFT_JIS\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
