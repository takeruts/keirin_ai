{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import math, os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング用の入力データの選択\n",
    "X_columns = ['locality', 'age', 'rank', 'leg', 'racing piont', \\\n",
    "             'S', 'B', 'Nige', 'Maki', 'Sashi', 'Ma', \\\n",
    "             '1st', '2nd', '3rd', 'Chakugai', 'win', '2ren', '3ren']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webスクレイピングで取得した戦績データをファイルから読み取り、データフレームに変換＋データ前処理\n",
    "def get_df_train(places):\n",
    "    \n",
    "    init_flag = True\n",
    "    for place in places:\n",
    "        print('loading data for ' + place)\n",
    "        filename = \"data/\" + place + \"_train_data.csv\"\n",
    "        df_train = pd.read_csv(filename, encoding=\"SHIFT_JIS\", header=0, nrows=None)\n",
    "\n",
    "        targets = []\n",
    "        name_ids = []\n",
    "        localities = []\n",
    "\n",
    "        for index, row in df_train.iterrows():\n",
    "\n",
    "            # 1位を予想するため One-Hot表現にする\n",
    "            result = row['result']\n",
    "            if result == 1:\n",
    "                target = 1\n",
    "            else:\n",
    "                target = 0\n",
    "            targets.append(target)    \n",
    "\n",
    "            # 名前をハッシュを使ってID化\n",
    "            name = row['name']\n",
    "            name_hash = hashlib.md5(name.encode()).hexdigest()\n",
    "            name_id = name_hash[-8:]\n",
    "            name_ids.append(name_id)\n",
    "\n",
    "            # 　ランクの例外処理\n",
    "            if row['rank'] == 'SS':\n",
    "                df_train.loc[index, 'rank'] = '0'\n",
    "            elif row['rank'] == 'L1':\n",
    "                df_train.loc[index, 'rank'] = '6'\n",
    "\n",
    "            # 出身地を地区毎にグループ化\n",
    "            prefecture = row['prefecture']\n",
    "            if prefecture in {'1', '2', '3', '5'}:\n",
    "                locality = '1' #北東北\n",
    "            elif prefecture in {'4', '6', '7'}:\n",
    "                locality = '2' #南東北\n",
    "            elif prefecture in {'8', '9'}:\n",
    "                locality = '3' #茨栃\n",
    "            elif prefecture in {'11', '13'}:\n",
    "                locality = '4' #埼京\n",
    "            elif prefecture in {'10', '15', '19', '20'}:\n",
    "                locality = '5' #上信越\n",
    "            elif prefecture in {'12', '14', '22'}:\n",
    "                locality = '6' #南関東\n",
    "            elif prefecture in {'16', '17', '21', '23', '24'}:\n",
    "                locality = '7' #中部\n",
    "            elif prefecture in {'18', '25', '26', '27', '28', '29', '30'}:\n",
    "                locality = '8' #近畿\n",
    "            elif prefecture in {'31', '32', '33', '34', '35'}:\n",
    "                locality = '9' #中国\n",
    "            elif prefecture in {'36', '37', '38', '39'}:\n",
    "                locality = '10' #四国\n",
    "            elif prefecture in {'40', '41', '42', '43', '44', '45', '46', '47'}:\n",
    "                locality = '11' #九州\n",
    "            else:\n",
    "                locality = '12' #外国\n",
    "\n",
    "            localities.append(locality)\n",
    "\n",
    "        # 前処理したデータのデータフレームへの置き換え\n",
    "        df_train['target'] = targets\n",
    "        df_train['name_id'] = name_ids\n",
    "        df_train['locality'] = localities\n",
    "\n",
    "        # カラムの順番入れ替え（見やすさのため）\n",
    "        columns = list(df_train.columns)\n",
    "        columns.remove('name_id')\n",
    "        columns.insert(columns.index(\"name\") + 1, \"name_id\")\n",
    "        columns.remove('locality')\n",
    "        columns.insert(columns.index(\"prefecture\") + 1, \"locality\")\n",
    "\n",
    "        df_train = df_train.loc[:,columns]\n",
    "        \n",
    "        if init_flag:\n",
    "            df_train_concat = df_train\n",
    "            init_flag = False\n",
    "        else:\n",
    "            df_train_concat = pd.concat([df_train_concat, df_train])\n",
    "    \n",
    "    return df_train_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df_train):\n",
    "    X = []\n",
    "    target = []\n",
    "        \n",
    "    # 各レース毎に\n",
    "    grouped = df_train.groupby(['date', 'place', 'race_num'])\n",
    "    for race_name, group in tqdm(grouped):\n",
    "        #print(race_name)\n",
    "        racer_count = group.shape[0]\n",
    "        # もし、９輪ではないレースは、トレーニングの対象から外す（モデルを固めるため）\n",
    "        if racer_count != 9:\n",
    "            continue\n",
    "        X.append(group[X_columns].values)\n",
    "        target.append(group['target'].values)\n",
    "\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "    d_ = np.array(target)\n",
    "\n",
    "    X_train, X_test, d_train, d_test = train_test_split(X, d_, test_size = 0.2)\n",
    "\n",
    "    return X_train, X_test, d_train, d_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#places.remove('iwakitaira')\n",
    "#print(\"Loading Training Data...\")\n",
    "#df_train = get_df_train(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self._x = None\n",
    "        self._y = None\n",
    "        self._t = None,\n",
    "        self._keep_prob = None\n",
    "        self._sess = None\n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        # He 初期化\n",
    "        n_sum = 1\n",
    "        for n in shape:\n",
    "            n_sum *= n\n",
    "        stddev = math.sqrt(2.0 / n_sum)\n",
    "        print('stddev: ', stddev)\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def inference(self, x, keep_prob):\n",
    "        # 入力層 - 隠れ層、隠れ層 - 隠れ層\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "\n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "\n",
    "            input = tf.layers.batch_normalization(input)\n",
    "            h = tf.nn.relu(tf.matmul(input, self.weights[-1]) + self.biases[-1])\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "        # 隠れ層 - 出力層\n",
    "        self.weights.append(self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "\n",
    "        y = tf.nn.softmax(tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def loss(self, y, t):\n",
    "        # クロスエントロピー  Nan 問題回避のためのコードに変更\n",
    "        #cross_entropy = tf.reduce_mean(-tf.reduce_sum(t * tf.log(y), axis=1))\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=t, logits=y))\n",
    "        #return cross_entropy\n",
    "        # L2 正則化\n",
    "        l2_decay = 0.0001\n",
    "        l2_losses = [tf.nn.l2_loss(w) for w in self.weights]\n",
    "        l2_loss = l2_decay * tf.add_n(l2_losses)\n",
    "        loss = cross_entropy + l2_loss\n",
    "        return loss\n",
    "\n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self, X_train, Y_train, nb_epoch=100, batch_size=100, p_keep=0.5, verbose=1):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prob\n",
    "\n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        self._y = y\n",
    "        self._sess = sess\n",
    "\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x: X_[start:end],\n",
    "                    t: Y_[start:end],\n",
    "                    keep_prob: p_keep\n",
    "                })\n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch:', epoch,\n",
    "                      ' loss:', loss_,\n",
    "                      ' accuracy:', accuracy_)\n",
    "\n",
    "        return self._history\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        accuracy = self.accuracy(self._y, self._t)\n",
    "        return accuracy.eval(session=self._sess, feed_dict={\n",
    "            self._x: X_test,\n",
    "            self._t: Y_test,\n",
    "            self._keep_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()  # 2つのプロットを関連付ける\n",
    "\n",
    "    ax1.plot(history['loss'], label='loss', color='orange')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_ylim(0, 2.5)\n",
    "    ax1.legend(loc='best', bbox_to_anchor=(1.01, 0.71, 0.322, .100), borderaxespad=0.,)\n",
    "\n",
    "    ax2.plot(history['accuracy'], label='accuracy', color='dodgerblue')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    ax2.legend(loc='best', bbox_to_anchor=(1.01, 0.8, 0.4, .100), borderaxespad=0.,)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aomori', 'beppu', 'chiba', 'fukui', 'gifu', 'hakodate', 'hiratsuka', 'hiroshima', 'hofu', 'ito', 'iwakitaira', 'kawasaki', 'keiokaku', 'kishiwada', 'kochi', 'kokura', 'komatsushima', 'kumamoto', 'kurume', 'maebashi', 'matsudo', 'matsusaka', 'matsuyama', 'mukomachi', 'nagoya', 'nara', 'odawara', 'ogaki', 'omiya', 'sasebo', 'seibuen', 'shizuoka', 'tachikawa', 'takamatsu', 'takeo', 'tamano', 'toride', 'toyama', 'toyohashi', 'utsunomiya', 'wakayama', 'yahiko', 'yokkaichi']\n",
      "loading data for aomori\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for beppu\n",
      "loading data for chiba\n",
      "loading data for fukui\n",
      "loading data for gifu\n",
      "loading data for hakodate\n",
      "loading data for hiratsuka\n",
      "loading data for hiroshima\n",
      "loading data for hofu\n",
      "loading data for ito\n",
      "loading data for kawasaki\n",
      "loading data for keiokaku\n",
      "loading data for kishiwada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for kochi\n",
      "loading data for kokura\n",
      "loading data for komatsushima\n",
      "loading data for kurume\n",
      "loading data for maebashi\n",
      "loading data for matsudo\n",
      "loading data for matsusaka\n",
      "loading data for matsuyama\n",
      "loading data for mukomachi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for nagoya\n",
      "loading data for nara\n",
      "loading data for odawara\n",
      "loading data for ogaki\n",
      "loading data for omiya\n",
      "loading data for sasebo\n",
      "loading data for seibuen\n",
      "loading data for shizuoka\n",
      "loading data for tachikawa\n",
      "loading data for takamatsu\n",
      "loading data for takeo\n",
      "loading data for tamano\n",
      "loading data for toride\n",
      "loading data for toyama\n",
      "loading data for toyohashi\n",
      "loading data for utsunomiya\n",
      "loading data for wakayama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for yahiko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for yokkaichi\n"
     ]
    }
   ],
   "source": [
    "places = []\n",
    "for filename in os.listdir('data/'):\n",
    "    place = filename.split('_')[0]\n",
    "    places.append(place)\n",
    "print(places)\n",
    "\n",
    "# クロスエントロピーが Nan になる場所を除外 (いわき平、熊本)\n",
    "places.remove('iwakitaira')\n",
    "places.remove('kumamoto')\n",
    "\n",
    "df_train = get_df_train(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th Loop\n",
      "Generating Training/Test Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 146536/146536 [01:06<00:00, 2189.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "stddev:  0.006944444444444444\n",
      "stddev:  0.005524271728019903\n",
      "stddev:  0.02946278254943948\n",
      "epoch: 0  loss: 2.0441735  accuracy: 0.3220472\n",
      "epoch: 1  loss: 2.0560544  accuracy: 0.31111997\n",
      "epoch: 2  loss: 2.0481548  accuracy: 0.32216683\n",
      "epoch: 3  loss: 2.0468009  accuracy: 0.32402793\n",
      "epoch: 4  loss: 2.0580027  accuracy: 0.3133267\n",
      "epoch: 5  loss: 2.0495658  accuracy: 0.3224061\n",
      "epoch: 6  loss: 2.0541127  accuracy: 0.31905615\n",
      "epoch: 7  loss: 2.0494747  accuracy: 0.32373545\n",
      "epoch: 8  loss: 2.0572872  accuracy: 0.3179794\n",
      "epoch: 9  loss: 2.0544636  accuracy: 0.31863078\n",
      "epoch: 10  loss: 2.0509443  accuracy: 0.3238551\n",
      "epoch: 11  loss: 2.057067  accuracy: 0.31883016\n",
      "epoch: 12  loss: 2.0580318  accuracy: 0.31772682\n",
      "epoch: 13  loss: 2.0556936  accuracy: 0.31851113\n",
      "epoch: 14  loss: 2.0687401  accuracy: 0.3078365\n",
      "epoch: 15  loss: 2.0663095  accuracy: 0.31076106\n",
      "epoch: 16  loss: 2.0584161  accuracy: 0.3198405\n",
      "epoch: 17  loss: 2.059275  accuracy: 0.3183915\n",
      "epoch: 18  loss: 2.0539525  accuracy: 0.32338983\n",
      "epoch: 19  loss: 2.057072  accuracy: 0.3201861\n",
      "epoch: 20  loss: 2.063684  accuracy: 0.31480226\n",
      "epoch: 21  loss: 2.0544205  accuracy: 0.3236557\n",
      "epoch: 22  loss: 2.0571568  accuracy: 0.3202127\n",
      "epoch: 23  loss: 2.0531797  accuracy: 0.3254902\n",
      "epoch: 24  loss: 2.0568902  accuracy: 0.32194084\n",
      "epoch: 25  loss: 2.0598013  accuracy: 0.31901628\n",
      "epoch: 26  loss: 2.0591538  accuracy: 0.31981388\n",
      "epoch: 27  loss: 2.0568314  accuracy: 0.32144898\n",
      "epoch: 28  loss: 2.0665824  accuracy: 0.3130874\n",
      "epoch: 29  loss: 2.064034  accuracy: 0.31587902\n",
      "epoch: 30  loss: 2.068036  accuracy: 0.3118511\n",
      "epoch: 31  loss: 2.0633354  accuracy: 0.31688935\n",
      "epoch: 32  loss: 2.0675902  accuracy: 0.31279495\n",
      "epoch: 33  loss: 2.0629673  accuracy: 0.31682286\n",
      "epoch: 34  loss: 2.0598974  accuracy: 0.3201861\n",
      "epoch: 35  loss: 2.0565176  accuracy: 0.32313725\n",
      "epoch: 36  loss: 2.0556457  accuracy: 0.32332337\n",
      "epoch: 37  loss: 2.0653481  accuracy: 0.31485543\n",
      "epoch: 38  loss: 2.0654888  accuracy: 0.31506813\n",
      "epoch: 39  loss: 2.0593016  accuracy: 0.32127616\n",
      "epoch: 40  loss: 2.0675528  accuracy: 0.31345963\n",
      "epoch: 41  loss: 2.0621753  accuracy: 0.31902957\n",
      "epoch: 42  loss: 2.058476  accuracy: 0.3232037\n",
      "epoch: 43  loss: 2.0637045  accuracy: 0.3173147\n",
      "epoch: 44  loss: 2.061904  accuracy: 0.31849784\n",
      "epoch: 45  loss: 2.0598772  accuracy: 0.321223\n",
      "epoch: 46  loss: 2.0706387  accuracy: 0.31073445\n",
      "epoch: 47  loss: 2.0660176  accuracy: 0.3151213\n",
      "epoch: 48  loss: 2.0582085  accuracy: 0.32289797\n",
      "epoch: 49  loss: 2.0653224  accuracy: 0.3161848\n",
      "epoch: 50  loss: 2.0723126  accuracy: 0.30956465\n",
      "epoch: 51  loss: 2.0777957  accuracy: 0.30426055\n",
      "epoch: 52  loss: 2.0734503  accuracy: 0.3088468\n",
      "epoch: 53  loss: 2.0592728  accuracy: 0.32279164\n",
      "epoch: 54  loss: 2.0674946  accuracy: 0.31444335\n",
      "epoch: 55  loss: 2.0683682  accuracy: 0.31360584\n",
      "epoch: 56  loss: 2.0603523  accuracy: 0.32097042\n",
      "epoch: 57  loss: 2.0583942  accuracy: 0.32372215\n",
      "epoch: 58  loss: 2.0627706  accuracy: 0.31944168\n",
      "epoch: 59  loss: 2.068464  accuracy: 0.31335327\n",
      "epoch: 60  loss: 2.083206  accuracy: 0.29903623\n",
      "epoch: 61  loss: 2.085398  accuracy: 0.29696244\n",
      "epoch: 62  loss: 2.0714805  accuracy: 0.31074774\n",
      "epoch: 63  loss: 2.069394  accuracy: 0.31270188\n",
      "epoch: 64  loss: 2.0698547  accuracy: 0.3126487\n",
      "epoch: 65  loss: 2.065473  accuracy: 0.31670323\n",
      "epoch: 66  loss: 2.0648587  accuracy: 0.3175407\n",
      "epoch: 67  loss: 2.0682495  accuracy: 0.31400466\n",
      "epoch: 68  loss: 2.0601766  accuracy: 0.3220472\n",
      "epoch: 69  loss: 2.0596476  accuracy: 0.32134265\n",
      "epoch: 70  loss: 2.0618646  accuracy: 0.31865737\n",
      "epoch: 71  loss: 2.0649405  accuracy: 0.31663677\n",
      "epoch: 72  loss: 2.0647256  accuracy: 0.31662345\n",
      "epoch: 73  loss: 2.0648937  accuracy: 0.3163443\n",
      "epoch: 74  loss: 2.066709  accuracy: 0.31514788\n",
      "epoch: 75  loss: 2.0654738  accuracy: 0.31703556\n",
      "epoch: 76  loss: 2.0659823  accuracy: 0.31641078\n",
      "epoch: 77  loss: 2.0615265  accuracy: 0.32067797\n",
      "epoch: 78  loss: 2.0755043  accuracy: 0.30726486\n",
      "epoch: 79  loss: 2.0639591  accuracy: 0.31867066\n",
      "epoch: 80  loss: 2.066751  accuracy: 0.31585243\n",
      "epoch: 81  loss: 2.0640912  accuracy: 0.31865737\n",
      "epoch: 82  loss: 2.072606  accuracy: 0.3102692\n",
      "epoch: 83  loss: 2.0726964  accuracy: 0.31002992\n",
      "epoch: 84  loss: 2.076178  accuracy: 0.30654702\n",
      "epoch: 85  loss: 2.069333  accuracy: 0.313659\n",
      "epoch: 86  loss: 2.0946703  accuracy: 0.28861415\n",
      "epoch: 87  loss: 2.0645745  accuracy: 0.31848454\n",
      "epoch: 88  loss: 2.0635364  accuracy: 0.319548\n",
      "epoch: 89  loss: 2.0715585  accuracy: 0.31131938\n",
      "epoch: 90  loss: 2.064558  accuracy: 0.3182054\n",
      "epoch: 91  loss: 2.0720224  accuracy: 0.31096044\n",
      "epoch: 92  loss: 2.0689003  accuracy: 0.31405783\n",
      "epoch: 93  loss: 2.0638046  accuracy: 0.31969425\n",
      "epoch: 94  loss: 2.070967  accuracy: 0.3122898\n",
      "epoch: 95  loss: 2.0680132  accuracy: 0.3149618\n",
      "epoch: 96  loss: 2.0632713  accuracy: 0.31974742\n",
      "epoch: 97  loss: 2.069947  accuracy: 0.31396478\n",
      "epoch: 98  loss: 2.0721483  accuracy: 0.31096044\n",
      "epoch: 99  loss: 2.0764644  accuracy: 0.306773\n",
      "epoch: 100  loss: 2.0756946  accuracy: 0.3078099\n",
      "epoch: 101  loss: 2.0638983  accuracy: 0.31958792\n",
      "epoch: 102  loss: 2.0718849  accuracy: 0.3115055\n",
      "epoch: 103  loss: 2.0643609  accuracy: 0.3193885\n",
      "epoch: 104  loss: 2.068458  accuracy: 0.31545365\n",
      "epoch: 105  loss: 2.0709336  accuracy: 0.31292787\n",
      "epoch: 106  loss: 2.067376  accuracy: 0.3165171\n",
      "epoch: 107  loss: 2.0712495  accuracy: 0.31279495\n",
      "epoch: 108  loss: 2.064737  accuracy: 0.31902957\n",
      "epoch: 109  loss: 2.070229  accuracy: 0.31375208\n",
      "epoch: 110  loss: 2.0684586  accuracy: 0.31562644\n",
      "epoch: 111  loss: 2.0693383  accuracy: 0.31462944\n",
      "epoch: 112  loss: 2.0738652  accuracy: 0.3102692\n",
      "epoch: 113  loss: 2.0705495  accuracy: 0.31356597\n",
      "epoch: 114  loss: 2.0688725  accuracy: 0.31544036\n",
      "epoch: 115  loss: 2.07393  accuracy: 0.31004322\n",
      "epoch: 116  loss: 2.0669332  accuracy: 0.31665006\n",
      "epoch: 117  loss: 2.0705154  accuracy: 0.31333998\n",
      "epoch: 118  loss: 2.0774693  accuracy: 0.3066401\n",
      "epoch: 119  loss: 2.0758653  accuracy: 0.30804917\n",
      "epoch: 120  loss: 2.0675256  accuracy: 0.31623796\n",
      "epoch: 121  loss: 2.067375  accuracy: 0.31641078\n",
      "epoch: 122  loss: 2.062738  accuracy: 0.32091725\n",
      "epoch: 123  loss: 2.063283  accuracy: 0.32043868\n",
      "epoch: 124  loss: 2.0652137  accuracy: 0.31836492\n",
      "epoch: 125  loss: 2.0710444  accuracy: 0.3127285\n",
      "epoch: 126  loss: 2.0647962  accuracy: 0.31889665\n",
      "epoch: 127  loss: 2.060622  accuracy: 0.32292455\n",
      "epoch: 128  loss: 2.077885  accuracy: 0.30596212\n",
      "epoch: 129  loss: 2.1011672  accuracy: 0.28276503\n",
      "epoch: 130  loss: 2.0729935  accuracy: 0.31110668\n",
      "epoch: 131  loss: 2.0905275  accuracy: 0.29345298\n",
      "epoch: 132  loss: 2.0703168  accuracy: 0.31337985\n",
      "epoch: 133  loss: 2.0719757  accuracy: 0.31191757\n",
      "epoch: 134  loss: 2.070351  accuracy: 0.31345963\n",
      "epoch: 135  loss: 2.0655687  accuracy: 0.31811234\n",
      "epoch: 136  loss: 2.062299  accuracy: 0.32089067\n",
      "epoch: 137  loss: 2.071512  accuracy: 0.31137255\n",
      "epoch: 138  loss: 2.0699883  accuracy: 0.31324694\n",
      "epoch: 139  loss: 2.0701547  accuracy: 0.31318045\n",
      "epoch: 140  loss: 2.0677269  accuracy: 0.31557328\n",
      "epoch: 141  loss: 2.0698082  accuracy: 0.31345963\n",
      "epoch: 142  loss: 2.0676887  accuracy: 0.31609172\n",
      "epoch: 143  loss: 2.0706732  accuracy: 0.312888\n",
      "epoch: 144  loss: 2.0729864  accuracy: 0.31062812\n",
      "epoch: 145  loss: 2.0713747  accuracy: 0.31197074\n",
      "epoch: 146  loss: 2.0699453  accuracy: 0.31381854\n",
      "epoch: 147  loss: 2.066173  accuracy: 0.3174078\n",
      "epoch: 148  loss: 2.0627139  accuracy: 0.32015952\n",
      "epoch: 149  loss: 2.0789003  accuracy: 0.30491194\n",
      "epoch: 150  loss: 2.0632098  accuracy: 0.32046527\n"
     ]
    }
   ],
   "source": [
    "for loop in range(1):\n",
    "    print(loop+1, \"th Loop\")\n",
    "    print(\"Generating Training/Test Data\")\n",
    "    X_train, X_test, Y_train, Y_test = get_train_test_data(df_train)\n",
    "\n",
    "    model = DNN(n_in = len(X_train[1]), n_hiddens=[256, 256], n_out=9)\n",
    "    print(\"Training ...\")\n",
    "    history = model.fit(X_train, Y_train, nb_epoch = 300, batch_size=32, p_keep=0.5)\n",
    "\n",
    "    accuracy = model.evaluate(X_test, Y_test)\n",
    "    print('accuracy: ', accuracy)\n",
    "    plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
