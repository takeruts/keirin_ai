{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "import math, os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング用の入力データの選択\n",
    "X_columns = ['locality', 'age', 'rank', 'leg', 'racing piont', \\\n",
    "             'S', 'B', 'Nige', 'Maki', 'Sashi', 'Ma', \\\n",
    "             '1st', '2nd', '3rd', 'Chakugai', 'win', '2ren', '3ren']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webスクレイピングで取得した戦績データをファイルから読み取り、データフレームに変換＋データ前処理\n",
    "def get_df_train(places):\n",
    "    \n",
    "    init_flag = True\n",
    "    for place in places:\n",
    "        print('loading data for ' + place)\n",
    "        filename = \"data/\" + place + \"_train_data.csv\"\n",
    "        df_train = pd.read_csv(filename, encoding=\"SHIFT_JIS\", header=0, nrows=None)\n",
    "\n",
    "        targets = []\n",
    "        name_ids = []\n",
    "        localities = []\n",
    "\n",
    "        for index, row in df_train.iterrows():\n",
    "\n",
    "            # 1位を予想するため One-Hot表現にする\n",
    "            result = row['result']\n",
    "            if result == 1:\n",
    "                target = 1\n",
    "            else:\n",
    "                target = 0\n",
    "            targets.append(target)    \n",
    "\n",
    "            # 名前をハッシュを使ってID化\n",
    "            name = row['name']\n",
    "            name_hash = hashlib.md5(name.encode()).hexdigest()\n",
    "            name_id = name_hash[-8:]\n",
    "            name_ids.append(name_id)\n",
    "\n",
    "            # 　ランクの例外処理\n",
    "            if row['rank'] == 'SS':\n",
    "                df_train.loc[index, 'rank'] = '0'\n",
    "            elif row['rank'] == 'L1':\n",
    "                df_train.loc[index, 'rank'] = '6'\n",
    "\n",
    "            # 出身地を地区毎にグループ化\n",
    "            prefecture = row['prefecture']\n",
    "            if prefecture in {'1', '2', '3', '5'}:\n",
    "                locality = '1' #北東北\n",
    "            elif prefecture in {'4', '6', '7'}:\n",
    "                locality = '2' #南東北\n",
    "            elif prefecture in {'8', '9'}:\n",
    "                locality = '3' #茨栃\n",
    "            elif prefecture in {'11', '13'}:\n",
    "                locality = '4' #埼京\n",
    "            elif prefecture in {'10', '15', '19', '20'}:\n",
    "                locality = '5' #上信越\n",
    "            elif prefecture in {'12', '14', '22'}:\n",
    "                locality = '6' #南関東\n",
    "            elif prefecture in {'16', '17', '21', '23', '24'}:\n",
    "                locality = '7' #中部\n",
    "            elif prefecture in {'18', '25', '26', '27', '28', '29', '30'}:\n",
    "                locality = '8' #近畿\n",
    "            elif prefecture in {'31', '32', '33', '34', '35'}:\n",
    "                locality = '9' #中国\n",
    "            elif prefecture in {'36', '37', '38', '39'}:\n",
    "                locality = '10' #四国\n",
    "            elif prefecture in {'40', '41', '42', '43', '44', '45', '46', '47'}:\n",
    "                locality = '11' #九州\n",
    "            else:\n",
    "                locality = '12' #外国\n",
    "\n",
    "            localities.append(locality)\n",
    "\n",
    "        # 前処理したデータのデータフレームへの置き換え\n",
    "        df_train['target'] = targets\n",
    "        df_train['name_id'] = name_ids\n",
    "        df_train['locality'] = localities\n",
    "\n",
    "        # カラムの順番入れ替え（見やすさのため）\n",
    "        columns = list(df_train.columns)\n",
    "        columns.remove('name_id')\n",
    "        columns.insert(columns.index(\"name\") + 1, \"name_id\")\n",
    "        columns.remove('locality')\n",
    "        columns.insert(columns.index(\"prefecture\") + 1, \"locality\")\n",
    "\n",
    "        df_train = df_train.loc[:,columns]\n",
    "        \n",
    "        if init_flag:\n",
    "            df_train_concat = df_train\n",
    "            init_flag = False\n",
    "        else:\n",
    "            df_train_concat = pd.concat([df_train_concat, df_train])\n",
    "    \n",
    "    return df_train_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df_train):\n",
    "    X = []\n",
    "    target = []\n",
    "        \n",
    "    # 各レース毎に\n",
    "    grouped = df_train.groupby(['date', 'place', 'race_num'])\n",
    "    for race_name, group in tqdm(grouped):\n",
    "        #print(race_name)\n",
    "        racer_count = group.shape[0]\n",
    "        # もし、９輪ではないレースは、トレーニングの対象から外す（モデルを固めるため）\n",
    "        if racer_count != 9:\n",
    "            continue\n",
    "        X.append(group[X_columns].values)\n",
    "        target.append(group['target'].values)\n",
    "\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "    d_ = np.array(target)\n",
    "\n",
    "    X_train, X_test, d_train, d_test = train_test_split(X, d_, test_size = 0.2)\n",
    "\n",
    "    return X_train, X_test, d_train, d_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aomori', 'beppu', 'chiba', 'fukui', 'gifu', 'hakodate', 'hiratsuka', 'hiroshima', 'hofu', 'ito', 'iwakitaira', 'kawasaki', 'keiokaku', 'kishiwada', 'kochi', 'kokura', 'komatsushima', 'kumamoto', 'kurume', 'maebashi', 'matsudo', 'matsusaka', 'matsuyama', 'mukomachi', 'nagoya', 'nara', 'odawara', 'ogaki', 'omiya', 'sasebo', 'seibuen', 'shizuoka', 'tachikawa', 'takamatsu', 'takeo', 'tamano', 'toride', 'toyama', 'toyohashi', 'utsunomiya', 'wakayama', 'yahiko', 'yokkaichi']\n"
     ]
    }
   ],
   "source": [
    "places = []\n",
    "for filename in os.listdir('data/'):\n",
    "    place = filename.split('_')[0]\n",
    "    places.append(place)\n",
    "print(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data...\n",
      "loading data for aomori\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taker\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2903: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for beppu\n",
      "loading data for chiba\n",
      "loading data for fukui\n",
      "loading data for gifu\n",
      "loading data for hakodate\n",
      "loading data for hiratsuka\n",
      "loading data for hiroshima\n",
      "loading data for hofu\n",
      "loading data for ito\n",
      "loading data for kawasaki\n",
      "loading data for keiokaku\n"
     ]
    }
   ],
   "source": [
    "places.remove('iwakitaira')\n",
    "print(\"Loading Training Data...\")\n",
    "df_train = get_df_train(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self, n_in, n_hiddens, n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self._x = None\n",
    "        self._y = None\n",
    "        self._t = None,\n",
    "        self._keep_prob = None\n",
    "        self._sess = None\n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "\n",
    "    def weight_variable(self, shape):\n",
    "        # He 初期化\n",
    "        n_sum = 1\n",
    "        for n in shape:\n",
    "            n_sum *= n\n",
    "        stddev = math.sqrt(2.0 / n_sum)\n",
    "        print('stddev: ', stddev)\n",
    "        initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def inference(self, x, keep_prob):\n",
    "        # 入力層 - 隠れ層、隠れ層 - 隠れ層\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "\n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "\n",
    "            input = tf.layers.batch_normalization(input)\n",
    "            h = tf.nn.relu(tf.matmul(input, self.weights[-1]) + self.biases[-1])\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "        # 隠れ層 - 出力層\n",
    "        self.weights.append(self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "\n",
    "        y = tf.nn.softmax(tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def loss(self, y, t):\n",
    "        # クロスエントロピー  Nan 問題回避のためのコードに変更\n",
    "        #cross_entropy = tf.reduce_mean(-tf.reduce_sum(t * tf.log(y), axis=1))\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=t, logits=y))\n",
    "        return cross_entropy\n",
    "        # L2 正則化\n",
    "        '''\n",
    "        l2_decay = 0.0001\n",
    "        l2_losses = [tf.nn.l2_loss(w) for w in self.weights]\n",
    "        l2_loss = l2_decay * tf.add_n(l2_losses)\n",
    "        loss = cross_entropy + l2_loss\n",
    "        return loss\n",
    "        '''\n",
    "\n",
    "    def training(self, loss):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    def fit(self, X_train, Y_train, nb_epoch=100, batch_size=100, p_keep=0.5, verbose=1):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self._keep_prob = keep_prob\n",
    "\n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        self._y = y\n",
    "        self._sess = sess\n",
    "\n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            X_, Y_ = shuffle(X_train, Y_train)\n",
    "\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x: X_[start:end],\n",
    "                    t: Y_[start:end],\n",
    "                    keep_prob: p_keep\n",
    "                })\n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch:', epoch,\n",
    "                      ' loss:', loss_,\n",
    "                      ' accuracy:', accuracy_)\n",
    "\n",
    "        return self._history\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        accuracy = self.accuracy(self._y, self._t)\n",
    "        return accuracy.eval(session=self._sess, feed_dict={\n",
    "            self._x: X_test,\n",
    "            self._t: Y_test,\n",
    "            self._keep_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()  # 2つのプロットを関連付ける\n",
    "\n",
    "    ax1.plot(history['loss'], label='loss', color='orange')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_ylim(0, 2.5)\n",
    "    ax1.legend(loc='best', bbox_to_anchor=(1.01, 0.71, 0.322, .100), borderaxespad=0.,)\n",
    "\n",
    "    ax2.plot(history['accuracy'], label='accuracy', color='dodgerblue')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    ax2.legend(loc='best', bbox_to_anchor=(1.01, 0.8, 0.4, .100), borderaxespad=0.,)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loop in range(10):\n",
    "    print(loop+1, \"th Loop\")\n",
    "    print(\"Generating Training/Test Data\")\n",
    "    X_train, X_test, Y_train, Y_test = get_train_test_data(df_train)\n",
    "\n",
    "    model = DNN(n_in = len(X_train[1]), n_hiddens=[256], n_out=9)\n",
    "    print(\"Training ...\")\n",
    "    history = model.fit(X_train, Y_train, nb_epoch = 120, batch_size=32, p_keep=1.0)\n",
    "\n",
    "    accuracy = model.evaluate(X_test, Y_test)\n",
    "    print('accuracy: ', accuracy)\n",
    "    plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
