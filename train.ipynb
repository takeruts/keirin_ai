{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/hakodate_train_data.csv\", encoding=\"SHIFT_JIS\", header=0, nrows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate targets for training (not using Result)\n",
    "targets = []\n",
    "for index, row in df_train.iterrows():\n",
    "    result = row['result']\n",
    "    target = (10 - result) / 45\n",
    "    targets.append(target)\n",
    "    \n",
    "df_train['target'] = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名前をハッシュを使ってID化\n",
    "name_ids = []\n",
    "for index, row in df_train.iterrows():\n",
    "    name = row['name']\n",
    "    name_hash = hashlib.md5(name.encode()).hexdigest()\n",
    "    name_id = name_hash[-8:]\n",
    "    name_ids.append(name_id)\n",
    "    \n",
    "df_train['name_id'] = name_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_train.iterrows():\n",
    "    if row['rank'] == 'SS':\n",
    "        df_train.loc[index, 'rank'] = '0'\n",
    "    elif row['rank'] == 'L1':\n",
    "        df_train.loc[index, 'rank'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['rank'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出身地を地区毎にグループ化\n",
    "localities = []\n",
    "for index, row in df_train.iterrows():\n",
    "    prefecture = row['prefecture']\n",
    "    if prefecture in {'1', '2', '3', '5'}:\n",
    "        locality = '1' #北東北\n",
    "    elif prefecture in {'4', '6', '7'}:\n",
    "        locality = '2' #南東北\n",
    "    elif prefecture in {'8', '9'}:\n",
    "        locality = '3' #茨栃\n",
    "    elif prefecture in {'11', '13'}:\n",
    "        locality = '4' #埼京\n",
    "    elif prefecture in {'10', '15', '19', '20'}:\n",
    "        locality = '5' #上信越\n",
    "    elif prefecture in {'12', '14', '22'}:\n",
    "        locality = '6' #南関東\n",
    "    elif prefecture in {'16', '17', '21', '23', '24'}:\n",
    "        locality = '7' #中部\n",
    "    elif prefecture in {'18', '25', '26', '27', '28', '29', '30'}:\n",
    "        locality = '8' #近畿\n",
    "    elif prefecture in {'31', '32', '33', '34', '35'}:\n",
    "        locality = '9' #中国\n",
    "    elif prefecture in {'36', '37', '38', '39'}:\n",
    "        locality = '10' #四国\n",
    "    else:\n",
    "        locality = '11' #九州\n",
    "    \n",
    "    localities.append(locality)\n",
    "\n",
    "df_train['locality'] = localities\n",
    "\n",
    "'''\n",
    "('北海道', '1').('青森', '2').('岩手', '3').('宮城', '4')\n",
    "('秋田', '5').('山形', '6').('福島', '7')\n",
    "('茨城', '8').('栃木', '9').('群馬', '10').('埼玉', '11').('千葉', '12').\n",
    "('東京', '13').('神奈川', '14').('新潟', '15').('富山', '16').('石川', '17').('福井', '18').\n",
    "('山梨', '19').('長野', '20').('岐阜', '21').('静岡', '22').('愛知', '23').('三重', '24').\n",
    "('滋賀', '25').('京都', '26').('大阪', '27').('兵庫', '28').('奈良', '29').('和歌山', '30').\n",
    "('鳥取', '31').('島根', '32').('岡山', '33').('広島', '34').('山口', '35').('徳島', '36').\n",
    "('香川', '37').('愛媛', '38').('高知', '39').('福岡', '40').('佐賀', '41').('長崎', '42').\n",
    "('熊本', '43').('大分', '44').('宮崎', '45').('鹿児島', '46').('沖縄', '47')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df_train.columns)\n",
    "columns.remove('name_id')\n",
    "columns.insert(columns.index(\"name\") + 1, \"name_id\")\n",
    "columns.remove('locality')\n",
    "columns.insert(columns.index(\"prefecture\") + 1, \"locality\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[:,columns]\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = ['locality', 'age', 'rank', 'leg', 'racing piont', 'S', 'B', 'Nige', 'Maki', 'Sashi', 'Ma', '1st', '2nd', '3rd', 'Chakugai', 'win', '2ren', '3ren']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters_num = 100\n",
    "batch_size = 100\n",
    "plot_interval = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 9 * len(X_columns)])\n",
    "d = tf.placeholder(tf.float32, [None, 9])\n",
    "W = tf.Variable(tf.random_normal([9 * len(X_columns), 9], stddev=0.01))\n",
    "b = tf.Variable(tf.zeros([9]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# 交差エントロピー\n",
    "cross_entropy = -tf.reduce_sum(d * tf.log(y), reduction_indices=[1])\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# 正誤を保存\n",
    "correct = tf.equal(tf.argmax(y, 1), tf.argmax(d, 1))\n",
    "# 正解率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_train.groupby(['date', 'place', 'race_num'])\n",
    "print(len(grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "race_count = 0\n",
    "for race_name, group in grouped:\n",
    "    print(race_name)\n",
    "    racer_count = group.shape[0]\n",
    "    if racer_count != 9:\n",
    "        continue\n",
    "\n",
    "    X = group[X_columns].values.reshape(1, -1)\n",
    "    target = group['target'].values.reshape(1,-1)\n",
    "    \n",
    "    sess.run(train, feed_dict={x: X, d: target})\n",
    "    print(sess.run(correct, feed_dict={x: X, d: target}))\n",
    "    accuracy_val = sess.run(accuracy, feed_dict={x: X, d: target})\n",
    "    accuracies.append(accuracy_val)\n",
    "    print('Generation: ' + str(i+1) + '. 正解率 = ' + str(accuracy_val))\n",
    "\n",
    "    race_count += 1\n",
    "    \n",
    "lists = range(0, race_count, plot_interval)\n",
    "plt.plot(lists, accuracies)\n",
    "plt.title(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
